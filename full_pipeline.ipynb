{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import selfies as sf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "import random\n",
    "\n",
    "# === 2. Dataset Preparation ===\n",
    "data = load_dataset(\"maykcaldas/smiles-transformers\", split=\"train\")\n",
    "smiles_list = data['smiles']\n",
    "logp_list = data['logP']\n",
    "\n",
    "# Convert to SELFIES & tokenize\n",
    "selfies_list = [sf.encoder(s) for s in smiles_list]\n",
    "\n",
    "# Normalize property (LogP for now)\n",
    "scaler = MinMaxScaler()\n",
    "logp_scaled = scaler.fit_transform([[p] for p in logp_list])\n",
    "\n",
    "# === 3. SELF-BART Molecule Encoder ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pschwllr/selfies-bart\")\n",
    "self_bart = AutoModelForSeq2SeqLM.from_pretrained(\"pschwllr/selfies-bart\")\n",
    "self_bart.eval()\n",
    "\n",
    "class SelfBARTEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = self_bart\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def forward(self, selfies_batch):\n",
    "        inputs = self.tokenizer(selfies_batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.model.model.encoder(**inputs)\n",
    "        return encoder_outputs.last_hidden_state[:, 0, :]  # CLS-like representation\n",
    "\n",
    "# === 4. Property Encoder ===\n",
    "class PropertyMLP(nn.Module):\n",
    "    def __init__(self, input_dim=1, latent_dim=768):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# === 5. Contrastive Loss ===\n",
    "def contrastive_loss(z_mol, z_prop, temp=0.07):\n",
    "    sim = F.cosine_similarity(z_mol.unsqueeze(1), z_prop.unsqueeze(0), dim=2)\n",
    "    logits = sim / temp\n",
    "    labels = torch.arange(len(z_mol)).to(z_mol.device)\n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_j = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i + loss_j) / 2\n",
    "\n",
    "# === 6. Training Loop ===\n",
    "mol_enc = SelfBARTEncoder()\n",
    "prop_enc = PropertyMLP()\n",
    "opt = torch.optim.Adam(prop_enc.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i in range(0, len(selfies_list), 32):\n",
    "        selfies_batch = selfies_list[i:i+32]\n",
    "        props = torch.tensor(logp_scaled[i:i+32], dtype=torch.float)\n",
    "        zmol = mol_enc(selfies_batch).detach()\n",
    "        zprop = prop_enc(props)\n",
    "\n",
    "        loss = contrastive_loss(zmol, zprop)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Step {i}, Loss {loss.item():.4f}\")\n",
    "\n",
    "# === 7. Inference ===\n",
    "def decode_from_latent(z):\n",
    "    # Project latent to decoder-compatible embedding (optional)\n",
    "    # Use random decoder tokens and force decode from z (needs custom logic if not exposed)\n",
    "    prompt = \"[C][C][O]\"  # temporary prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = self_bart.generate(**inputs, max_length=40)\n",
    "    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return decoded\n",
    "\n",
    "test_prop = torch.tensor([[0.6]])\n",
    "z_test = prop_enc(test_prop)\n",
    "generated_selfies = decode_from_latent(z_test)\n",
    "print(\"Generated SELFIES:\", generated_selfies)\n",
    "print(\"Decoded SMILES:\", sf.decoder(generated_selfies))\n",
    "\n",
    "# === 8. Evaluation ===\n",
    "def is_valid(smiles):\n",
    "    return Chem.MolFromSmiles(smiles) is not None\n",
    "\n",
    "def compute_logp(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return Descriptors.MolLogP(mol) if mol else None\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
